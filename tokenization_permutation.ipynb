{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import spacy\n",
    "import string\n",
    "import torch\n",
    "import copy\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import pickle as pkl\n",
    "from nltk import ngrams\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = '/Users/yi/Documents/DS1011/hw1/data/aclImdb/train'\n",
    "test_path = '/Users/yi/Documents/DS1011/hw1/data/aclImdb/test'\n",
    "\n",
    "def readfiles(path):\n",
    "    signset = ['pos','neg']\n",
    "    dataset = []\n",
    "    targets = []\n",
    "    for sign in signset: \n",
    "        files = os.listdir(path+'/' + sign)\n",
    "        if sign == 'pos':\n",
    "            target = 1\n",
    "        else:\n",
    "            target = 0\n",
    "        for file in files:\n",
    "            f = open(path+'/'+sign +'/'+file)\n",
    "            iter_f = iter(f)\n",
    "            tmp = ''\n",
    "            for line in iter_f:\n",
    "                tmp += line\n",
    "            dataset.append(tmp)\n",
    "            targets.append(target)\n",
    "    return [dataset,targets]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x,train_y = readfiles(train_path)\n",
    "train_x,val_x,train_y,val_y = train_test_split(train_x, train_y, test_size=0.2,random_state = 1)\n",
    "test_x,test_y = readfiles(test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = spacy.load('en_core_web_sm')\n",
    "punctuations = string.punctuation\n",
    "# to delete common words\n",
    "def isCommon(ngram):\n",
    "    commonWords = [\"the\", \"be\", \"and\", \"of\", \"a\", \"in\", \"to\", \"have\",\n",
    "                   \"it\", \"i\", \"that\", \"for\", \"you\", \"he\", \"with\", \"on\", \"do\", \"say\",\n",
    "                   \"this\", \"they\", \"is\", \"an\", \"at\", \"but\",\"we\", \"his\", \"from\", \"that\",\n",
    "                   \"by\", \"she\", \"or\", \"as\", \"what\", \"go\", \"their\",\"can\", \"who\",\n",
    "                   \"get\", \"if\", \"would\", \"her\", \"all\", \"my\", \"make\", \"about\", \"know\",\n",
    "                   \"will\",\"as\", \"up\", \"one\", \"time\", \"has\", \"been\", \"there\", \"year\", \"so\",\n",
    "                   \"think\", \"when\", \"which\", \"them\", \"some\", \"me\", \"people\", \"take\", \"out\",\n",
    "                   \"into\", \"just\", \"see\", \"him\", \"your\", \"come\", \"could\", \"now\", \"than\",\n",
    "                   \"like\", \"other\", \"how\", \"then\", \"its\", \"our\", \"two\", \"more\", \"these\",\n",
    "                   \"want\", \"way\", \"look\", \"first\", \"also\", \"new\", \"because\", \"day\", \"more\",\n",
    "                   \"use\", \"man\", \"find\", \"here\", \"thing\", \"give\", \"many\", \"well\"]\n",
    "\n",
    "    return True if ngram in commonWords else False\n",
    "\n",
    "# lowercase and remove punctuation \n",
    "def tokenize(sent):\n",
    "    tokens = tokenizer(sent)\n",
    "    return [token.text.lower() for token in tokens if (token.text not in punctuations)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_ngram_common(dataset,n):\n",
    "    token_dataset = []\n",
    "    all_tokens = []\n",
    "    for sample in dataset:\n",
    "        tokens = tokenize(sample)\n",
    "        if n == 1:\n",
    "            token_dataset.append(tokens)\n",
    "            all_tokens += tokens\n",
    "        else:\n",
    "            ngram_tokens = ngrams(tokens,n)\n",
    "            tmp = [c for c in ngram_tokens]\n",
    "            token_dataset.append(tmp)\n",
    "            all_tokens += tmp\n",
    "    return token_dataset, all_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_tokens_common, all_train_tokens_common = tokenize_ngram_common(train_x,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_data_tokens_common, _ = tokenize_ngram_common(val_x,1)\n",
    "test_data_tokens_common, _ = tokenize_ngram_common(test_x,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocab(all_tokens,max_vocab_size = 10000, PAD_IDX = 0,UNK_IDX = 1):\n",
    "    # Returns:\n",
    "    # id2token: list of tokens, where id2token[i] returns token that corresponds to token i\n",
    "    # token2id: dictionary where keys represent tokens and corresponding values represent indices\n",
    "    token_counter = Counter(all_tokens)\n",
    "    vocab, count = zip(*token_counter.most_common(max_vocab_size))\n",
    "    id2token = list(vocab)\n",
    "    token2id = dict(zip(vocab, range(2,2+len(vocab)))) \n",
    "    id2token = ['<pad>', '<unk>'] + id2token\n",
    "    token2id['<pad>'] = PAD_IDX \n",
    "    token2id['<unk>'] = UNK_IDX\n",
    "    return token2id, id2token\n",
    "def token2index_dataset(tokens_data):\n",
    "    indices_data = []\n",
    "    for tokens in tokens_data:\n",
    "        for token in tokens:\n",
    "            index_list = []\n",
    "            if token in token2id:\n",
    "                index_list.append(token2id[token])\n",
    "            else:\n",
    "                index_list.append(1)\n",
    "        indices_data.append(index_list)\n",
    "    return indices_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SENTENCE_LENGTH = 200\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class NewsGroupDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Class that represents a train/validation/test dataset that's readable for PyTorch\n",
    "    Note that this class inherits torch.utils.data.Dataset\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, data_list, target_list):\n",
    "        \"\"\"\n",
    "        @param data_list: list of newsgroup tokens \n",
    "        @param target_list: list of newsgroup targets \n",
    "\n",
    "        \"\"\"\n",
    "        self.data_list = data_list\n",
    "        self.target_list = target_list\n",
    "        assert (len(self.data_list) == len(self.target_list))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_list)\n",
    "        \n",
    "    def __getitem__(self, key):\n",
    "        \"\"\"\n",
    "        Triggered when you call dataset[i]\n",
    "        \"\"\"\n",
    "        \n",
    "        token_idx = self.data_list[key][:MAX_SENTENCE_LENGTH]\n",
    "        label = self.target_list[key]\n",
    "        return [token_idx, len(token_idx), label]\n",
    "\n",
    "def newsgroup_collate_func(batch):\n",
    "    \"\"\"\n",
    "    Customized function for DataLoader that dynamically pads the batch so that all \n",
    "    data have the same length\n",
    "    \"\"\"\n",
    "    data_list = []\n",
    "    label_list = []\n",
    "    length_list = []\n",
    "    #print(\"collate batch: \", batch[0][0])\n",
    "    #batch[0][0] = batch[0][0][:MAX_SENTENCE_LENGTH]\n",
    "    for datum in batch:\n",
    "        label_list.append(datum[2])\n",
    "        length_list.append(datum[1])\n",
    "    # padding\n",
    "    for datum in batch:\n",
    "        padded_vec = np.pad(np.array(datum[0]), \n",
    "                                pad_width=((0,MAX_SENTENCE_LENGTH-datum[1])), \n",
    "                                mode=\"constant\", constant_values=0)\n",
    "        data_list.append(padded_vec)\n",
    "    return [torch.from_numpy(np.array(data_list)), torch.LongTensor(length_list), torch.LongTensor(label_list)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Ngrams(nn.Module):\n",
    "    #2-class classification\n",
    "    def __init__(self, vocab_size, emb_dim):\n",
    "        \"\"\"\n",
    "        @param vocab_size: size of the vocabulary. \n",
    "        @param emb_dim: size of the word embedding\n",
    "        \"\"\"\n",
    "        super(Ngrams, self).__init__()\n",
    "        # pay attention to padding_idx \n",
    "        self.embed = nn.Embedding(vocab_size, emb_dim, padding_idx=0)\n",
    "        self.linear = nn.Linear(emb_dim,2)\n",
    "    \n",
    "    def forward(self, data, length):\n",
    "        \"\"\"\n",
    "        \n",
    "        @param data: matrix of size (batch_size, max_sentence_length). Each row in data represents a \n",
    "            review that is represented using n-gram index. Note that they are padded to have same length.\n",
    "        @param length: an int tensor of size (batch_size), which represents the non-trivial (excludes padding)\n",
    "            length of each sentences in the data.\n",
    "        \"\"\"\n",
    "        out = self.embed(data)\n",
    "        out = torch.sum(out, dim=1)\n",
    "        out /= length.view(length.size()[0],1).expand_as(out).float()\n",
    "     \n",
    "        # return logits\n",
    "        out = self.linear(out.float())\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(loader, model):\n",
    "    \"\"\"\n",
    "    Help function that tests the model's performance on a dataset\n",
    "    @param: loader - data loader for the dataset to test against\n",
    "    \"\"\"\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    model.eval()\n",
    "    for data, lengths, labels in loader:\n",
    "        data_batch, length_batch, label_batch = data, lengths, labels\n",
    "        outputs = F.softmax(model(data_batch, length_batch), dim=1)\n",
    "        predicted = outputs.max(1, keepdim=True)[1]       \n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels.view_as(predicted)).sum().item()\n",
    "    return (100 * correct / total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1/5], Step: [101/625], Validation Acc: 56.38\n",
      "Epoch: [1/5], Step: [201/625], Validation Acc: 56.22\n",
      "Epoch: [1/5], Step: [301/625], Validation Acc: 56.72\n",
      "Epoch: [1/5], Step: [401/625], Validation Acc: 58.24\n",
      "Epoch: [1/5], Step: [501/625], Validation Acc: 58.06\n",
      "Epoch: [1/5], Step: [601/625], Validation Acc: 58.46\n",
      "Epoch: [2/5], Step: [101/625], Validation Acc: 58.82\n",
      "Epoch: [2/5], Step: [201/625], Validation Acc: 58.68\n",
      "Epoch: [2/5], Step: [301/625], Validation Acc: 59.54\n",
      "Epoch: [2/5], Step: [401/625], Validation Acc: 60.34\n",
      "Epoch: [2/5], Step: [501/625], Validation Acc: 60.92\n",
      "Epoch: [2/5], Step: [601/625], Validation Acc: 58.16\n",
      "Epoch: [3/5], Step: [101/625], Validation Acc: 59.92\n",
      "Epoch: [3/5], Step: [201/625], Validation Acc: 59.28\n",
      "Epoch: [3/5], Step: [301/625], Validation Acc: 60.08\n",
      "Epoch: [3/5], Step: [401/625], Validation Acc: 60.98\n",
      "Epoch: [3/5], Step: [501/625], Validation Acc: 60.58\n",
      "Epoch: [3/5], Step: [601/625], Validation Acc: 59.98\n",
      "Epoch: [4/5], Step: [101/625], Validation Acc: 59.54\n",
      "Epoch: [4/5], Step: [201/625], Validation Acc: 59.0\n",
      "Epoch: [4/5], Step: [301/625], Validation Acc: 59.94\n",
      "Epoch: [4/5], Step: [401/625], Validation Acc: 60.08\n",
      "Epoch: [4/5], Step: [501/625], Validation Acc: 60.1\n",
      "Epoch: [4/5], Step: [601/625], Validation Acc: 59.7\n",
      "Epoch: [5/5], Step: [101/625], Validation Acc: 59.48\n",
      "Epoch: [5/5], Step: [201/625], Validation Acc: 60.02\n",
      "Epoch: [5/5], Step: [301/625], Validation Acc: 59.76\n",
      "Epoch: [5/5], Step: [401/625], Validation Acc: 59.84\n",
      "Epoch: [5/5], Step: [501/625], Validation Acc: 59.62\n",
      "Epoch: [5/5], Step: [601/625], Validation Acc: 59.6\n"
     ]
    }
   ],
   "source": [
    "train_data_tokens = pkl.load(open(\"train_data_tokens_1.p\", \"rb\"))\n",
    "all_train_tokens = pkl.load(open(\"all_train_tokens_1.p\", \"rb\"))\n",
    "\n",
    "val_data_tokens = pkl.load(open(\"val_data_tokens_1.p\", \"rb\"))\n",
    "test_data_tokens = pkl.load(open(\"test_data_tokens_1.p\", \"rb\"))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([84.5,\n",
       "  85.86,\n",
       "  88.74000000000001,\n",
       "  87.4,\n",
       "  88.03999999999999,\n",
       "  88.25999999999999,\n",
       "  88.52000000000001,\n",
       "  88.6,\n",
       "  87.92,\n",
       "  87.7,\n",
       "  88.53999999999999,\n",
       "  88.9,\n",
       "  88.22,\n",
       "  88.46000000000001,\n",
       "  88.08,\n",
       "  88.86,\n",
       "  89.06,\n",
       "  88.34,\n",
       "  88.6,\n",
       "  88.68,\n",
       "  88.82,\n",
       "  89.52000000000001,\n",
       "  88.4,\n",
       "  88.4,\n",
       "  89.34,\n",
       "  87.86,\n",
       "  88.88,\n",
       "  88.42,\n",
       "  86.82,\n",
       "  87.64],\n",
       " [8.424,\n",
       "  8.067,\n",
       "  7.707,\n",
       "  7.827,\n",
       "  7.694,\n",
       "  7.3260000000000005,\n",
       "  7.013999999999999,\n",
       "  6.994999999999999,\n",
       "  6.827000000000002,\n",
       "  6.811000000000002,\n",
       "  6.7940000000000005,\n",
       "  6.541,\n",
       "  6.66,\n",
       "  6.6530000000000005,\n",
       "  6.5840000000000005,\n",
       "  6.505000000000002,\n",
       "  6.316999999999999,\n",
       "  6.594999999999999,\n",
       "  6.187000000000001,\n",
       "  6.297999999999999,\n",
       "  6.404,\n",
       "  6.313,\n",
       "  6.395,\n",
       "  6.388,\n",
       "  6.263999999999999,\n",
       "  6.3610000000000015,\n",
       "  6.2620000000000005,\n",
       "  6.515000000000001,\n",
       "  6.6530000000000005,\n",
       "  6.511000000000002])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_vocab_size = 10000\n",
    "token2id, id2token = build_vocab(all_train_tokens,max_vocab_size = max_vocab_size )\n",
    "train_data_indices = token2index_dataset(train_data_tokens)\n",
    "val_data_indices = token2index_dataset(val_data_tokens)\n",
    "test_data_indices = token2index_dataset(test_data_tokens)\n",
    "BATCH_SIZE = 32\n",
    "MAX_SENTENCE_LENGTH = 200\n",
    "train_dataset = NewsGroupDataset(train_data_indices, train_y)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=newsgroup_collate_func,\n",
    "                                           shuffle=True)\n",
    "\n",
    "val_dataset = NewsGroupDataset(val_data_indices, val_y)\n",
    "val_loader = torch.utils.data.DataLoader(dataset=val_dataset, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=newsgroup_collate_func,\n",
    "                                           shuffle=True)\n",
    "\n",
    "test_dataset = NewsGroupDataset(test_data_indices, test_y)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=newsgroup_collate_func,\n",
    "                                           shuffle=False)\n",
    "\n",
    "emb_dim = 400\n",
    "model = Ngrams(len(id2token), emb_dim)\n",
    "learning_rate = 0.005\n",
    "num_epochs = 5 # number epoch to train\n",
    "\n",
    "# Criterion and Optimizer\n",
    "criterion = torch.nn.CrossEntropyLoss()  \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "val_acclist = []\n",
    "train_losslist = []\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (data, lengths, labels) in enumerate(train_loader):\n",
    "        model.train()\n",
    "        data_batch, length_batch, label_batch = data, lengths, labels\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(data_batch, length_batch)\n",
    "        loss = criterion(outputs, label_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # validate every 100 iterations\n",
    "        if i > 0 and i % 100 == 0:\n",
    "            # validate\n",
    "            val_acc = test_model(val_loader, model)\n",
    "            val_acclist.append(val_acc)\n",
    "            train_losslist.append((100.0 - test_model(train_loader, model)))\n",
    "val_acclist,train_losslist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
